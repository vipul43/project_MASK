{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipul43/project_MASK/blob/main/basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv-Nt1N5YOGC"
      },
      "source": [
        "# importing dataset from kaggle\n",
        "!pip install -q kaggle &> /dev/null\n",
        "!mkdir ~/.kaggle &> /dev/null\n",
        "!echo '{\"username\":\"saivipul\",\"key\":\"f4e9e153799bf7289426d9655d02fdca\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json &> /dev/null\n",
        "# enter dataset API command below\n",
        "!kaggle datasets download -d ashishjangra27/face-mask-12k-images-dataset &> /dev/null\n",
        "!unzip face-mask-12k-images-dataset.zip &> /dev/null\n",
        "!rm -rf face-mask-12k-images-dataset.zip &> /dev/null\n",
        "\n",
        "# dataset is stored in 'Face Mask Dataset' folder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdNqnIK5evdc"
      },
      "source": [
        "train_images_with_mask_path = '/content/Face Mask Dataset/Train/WithMask/'\n",
        "train_images_without_mask_path = '/content/Face Mask Dataset/Train/WithoutMask/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "GHY3n4gXe9bs",
        "outputId": "97b1f761-9b93-4b51-a8b9-9bc1393313ed"
      },
      "source": [
        "# playing around with data\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "image = load_img(train_images_with_mask_path+'10'+'.png', color_mode=\"grayscale\", target_size=None, interpolation=\"nearest\")\n",
        "display(image)\n",
        "print(type(image))\n",
        "print(image.format)\n",
        "print(image.mode)\n",
        "print(image.size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=82x82 at 0x7F5C9ECF3438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'PIL.Image.Image'>\n",
            "None\n",
            "L\n",
            "(82, 82)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EpwuTzrk1G0",
        "outputId": "47eb2221-1904-438b-fe0b-290905bc9d79"
      },
      "source": [
        "# converting image to vector\n",
        "image_vec = img_to_array(image)\n",
        "print(type(image_vec))\n",
        "print(image_vec.dtype)\n",
        "print(image_vec.shape)\n",
        "print(image_vec)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "float32\n",
            "(82, 82, 1)\n",
            "[[[238.]\n",
            "  [238.]\n",
            "  [238.]\n",
            "  ...\n",
            "  [199.]\n",
            "  [200.]\n",
            "  [200.]]\n",
            "\n",
            " [[237.]\n",
            "  [238.]\n",
            "  [240.]\n",
            "  ...\n",
            "  [187.]\n",
            "  [194.]\n",
            "  [195.]]\n",
            "\n",
            " [[237.]\n",
            "  [238.]\n",
            "  [240.]\n",
            "  ...\n",
            "  [196.]\n",
            "  [193.]\n",
            "  [194.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[236.]\n",
            "  [235.]\n",
            "  [235.]\n",
            "  ...\n",
            "  [152.]\n",
            "  [151.]\n",
            "  [152.]]\n",
            "\n",
            " [[236.]\n",
            "  [236.]\n",
            "  [235.]\n",
            "  ...\n",
            "  [152.]\n",
            "  [151.]\n",
            "  [152.]]\n",
            "\n",
            " [[235.]\n",
            "  [235.]\n",
            "  [234.]\n",
            "  ...\n",
            "  [151.]\n",
            "  [151.]\n",
            "  [152.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMp2c1jpnmag",
        "outputId": "e4a84f8b-0d24-47b9-e640-8cd5b4762e7a"
      },
      "source": [
        "# loading entire images dataset\n",
        "import tensorflow as tf\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/Face Mask Dataset/Train',\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"binary\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    image_size=(64, 64),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation=\"bilinear\",\n",
        "    follow_links=False,\n",
        ")\n",
        "print(type(train_dataset))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 files belonging to 2 classes.\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Bir2rLxxRx",
        "outputId": "b3b2d0e5-6c76-48b0-83c1-e3ebad2121fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for image, label in train_dataset:\n",
        "  print(image)\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[155.67676    124.23926     74.55371   ]\n",
            "   [160.38672    130.63672     80.74902   ]\n",
            "   [195.2832     164.6582     115.08496   ]\n",
            "   ...\n",
            "   [ 77.725586    52.808594    12.683594  ]\n",
            "   [118.4541      85.51074     46.916992  ]\n",
            "   [155.07227    117.197266    77.041016  ]]\n",
            "\n",
            "  [[148.24121    120.53516     68.49121   ]\n",
            "   [167.97949    139.93066     89.930664  ]\n",
            "   [193.1289     164.94336    114.94336   ]\n",
            "   ...\n",
            "   [102.19043     75.40918     35.660156  ]\n",
            "   [ 88.65918     54.10742     17.083008  ]\n",
            "   [160.21484    121.62793     83.146484  ]]\n",
            "\n",
            "  [[135.38965    108.27539     55.618164  ]\n",
            "   [175.81152    150.09277     97.46777   ]\n",
            "   [193.08984    166.60742    115.84863   ]\n",
            "   ...\n",
            "   [131.44824    102.26074     63.61328   ]\n",
            "   [ 99.33984     63.839844    27.404297  ]\n",
            "   [114.43848     75.65625     38.208984  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 79.66504     40.35254      5.040039  ]\n",
            "   [ 95.28516     59.723633    20.692383  ]\n",
            "   [103.95117     72.65234     33.442383  ]\n",
            "   ...\n",
            "   [ 47.066406    27.548828     3.790039  ]\n",
            "   [ 55.503906    32.50586      7.1953125 ]\n",
            "   [ 65.543945    40.825195    11.387695  ]]\n",
            "\n",
            "  [[ 85.018555    45.549805     5.8310547 ]\n",
            "   [ 98.12891     60.743164    18.479492  ]\n",
            "   [114.40918     83.03418     40.12793   ]\n",
            "   ...\n",
            "   [ 44.5625      25.560547     2.5       ]\n",
            "   [ 51.79883     29.55957      5.90625   ]\n",
            "   [ 60.404297    35.70508      6.692383  ]]\n",
            "\n",
            "  [[ 89.57129     47.773438     6.8066406 ]\n",
            "   [101.024414    62.749023    16.780273  ]\n",
            "   [115.896484    83.51074     37.90625   ]\n",
            "   ...\n",
            "   [ 38.3125      20.3125       0.22851562]\n",
            "   [ 39.53125     17.6875       0.        ]\n",
            "   [ 56.1416      31.547852     5.03125   ]]]], shape=(1, 64, 64, 3), dtype=float32)\n",
            "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65EJtO4vtIwD"
      },
      "source": [
        "from keras import Input\n",
        "from keras.layers import Conv2D, Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "def make_model(input_shape):\n",
        "  inputs = Input(shape=input_shape)\n",
        "  x = Conv2D(\n",
        "      filters=2,\n",
        "      kernel_size=3,\n",
        "      strides=(1, 1),\n",
        "      padding=\"valid\",\n",
        "      data_format=None,\n",
        "      dilation_rate=(1, 1),\n",
        "      groups=1,\n",
        "      activation='relu',\n",
        "      use_bias=True,\n",
        "      kernel_initializer=\"glorot_uniform\",\n",
        "      bias_initializer=\"zeros\",\n",
        "      kernel_regularizer=None,\n",
        "      bias_regularizer=None,\n",
        "      activity_regularizer=None,\n",
        "      kernel_constraint=None,\n",
        "      bias_constraint=None,\n",
        "  )(inputs)\n",
        "  x = Flatten()(x)\n",
        "  outputs = Dense(\n",
        "      1,\n",
        "      activation='sigmoid',\n",
        "      use_bias=True,\n",
        "      kernel_initializer=\"glorot_uniform\",\n",
        "      bias_initializer=\"zeros\",\n",
        "      kernel_regularizer=None,\n",
        "      bias_regularizer=None,\n",
        "      activity_regularizer=None,\n",
        "      kernel_constraint=None,\n",
        "      bias_constraint=None,\n",
        "  )(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model_basic.png')\n",
        "  return model"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScLL8ySnCW2q",
        "outputId": "e00410d1-0faf-417f-a8fb-e74bee0582e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model((64, 64, 3))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "model.fit(train_dataset, epochs=10, steps_per_epoch=1, verbose=1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 62, 62, 2)         56        \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 7688)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 7689      \n",
            "=================================================================\n",
            "Total params: 7,745\n",
            "Trainable params: 7,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 1.0013e-25\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3279e-13\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0209\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 679.1632\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6058e-37\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 158.8372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c536bec18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4irxDZkO69k"
      },
      "source": [
        "test_images_with_mask_path = '/content/Face Mask Dataset/Test/WithMask/'\n",
        "test_images_without_mask_path = '/content/Face Mask Dataset/Test/WithoutMask/'"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PaG-BxXG_te",
        "outputId": "c92c1766-a60f-457d-a94a-ca79305fa3ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "image = load_img(test_images_with_mask_path+'1163'+'.png', color_mode=\"rgb\", target_size=(64, 64), interpolation=\"nearest\")\n",
        "image_vec = img_to_array(image)\n",
        "image_vec = np.expand_dims(image_vec, axis=0)\n",
        "print(image_vec.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tf_B5iMO0es",
        "outputId": "f0498c92-50b1-4dcb-e5a7-d074243dc219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "yhat = model.predict([image_vec], verbose=1)\n",
        "print(yhat)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n",
            "[[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa5wmDehPYKj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}